This is a mirror of http://www.ieeetcsc.org/activities/blog/challenges_for_interoperability_of_runtime_systems_in_scientific_applications because that website had an invalid HTTPS certificate and now has disappeared from the internet.

# Challenges for Interoperability of Runtime Systems in Scientific Applications

Date: August 6th, 2012

Application development in high-performance computing (HPC) is growing increasingly complex in many different directions.  The constrains of power and the end of frequency scaling demand a move to multicore and heterogeneous architectures with ever deeper memory hierarchies.

Networks have grown increasingly complex and now must span as many as 100,000 nodes (e.g. the Sequoia Blue Gene/Q system that currently tops the [Top500](https://www.top500.org/list/2012/06/) list) and low-level communication software implements a number of advanced features not found in Ethernet. 

To deal with these challenges, application developers employ one or more runtime systems to manage the intra- and inter-node complexity of HPC systems.  The most common runtime system for parallel applications is MPI, but one observes less common usage of other systems, e.g. [UPC](http://upc.lbl.gov/), [Global Arrays](https://www.emsl.pnl.gov/docs/global/)  and [Charm++](http://charm.cs.uiuc.edu/).  MPI applications exist in a variety of forms, the simplest being those codes where MPI is used directly in the application, while others use MPI through high-level libraries such as [PETSc](https://www.mcs.anl.gov/petsc/) or [Elemental](http://libelemental.org/).  Even UPC, Global Arrays and Charm++ can exist as high-level libraries on top of MPI, but their high-performance implementation usually entails bypassing MPI and accessing a lower level network protocol.

In developing production and prototype applications for current and next-generation supercomputers, it has been observed that there are difficult challenges associated with runtime interoperability.  The purpose of this post is to describe these challenges and their potential solution so that others can workaround them as they emerge or design strategies to avoid them altogether as they evolve their applications for ever more complex software and hardware. 

[MADNESS](https://github.com/m-a-d-n-e-s-s/madness) is a modern scientific software package for solving partial differential equations using multi resolution methods.  One of its uses is quantum chemistry, which entails the solution of the Schroedinger equation using a number of numerical and physical approximations.  Of keen interest is density-functional theory (DFT), which is extremely popular due to its favorable ratio of accuracy to computational cost relative to explicit wave function methods.  In the course of solving the DFT equations, MADNESS uses a dense eigensolver to obtain the initial guess wave function, among other things.  A natural choice to perform the dense eigensolve is the extremely scalable library, Elemental, which is naturally suited for MADNESS due to its elegant design in C++, use of hybrid programming models (MPI with OpenMP and Pthreads), and excellent efficiency on supercomputing architectures such as Blue Gene/P.

MADNESS uses a completely asynchronous threaded runtime built upon Pthreads.  When threads need to send messages, they not only access MPI, but shared data structures that require a mutex, hence MADNESS uses MPI_THREAD_SERIALIZED (henceforth, SERIALIZED) to avoid an unnecessary second lock inside of MPI that is implied by MULTIPLE (henceforth, MULTIPLE). This has the advantage of improving performance, especially when the MPI implementation is not able to provide MULTIPLE in an efficient way.  The disadvantage is that MADNESS is continuously calling MPI from threads and any external library that attempts to use MPI directly, as Elemental does, will violate the serialization requirement that is satisfied internally by MADNESS.

There are a number of possible solutions to this problem.  Obviously, one could quiesce all the MADNESS threads that can call MPI before switching to an external library that needs to call MPI, but this has the effect of a global barrier, which is exactly the opposite of what one strives for in an asynchronous runtime such as MADNESS.  It precludes the possibility of overlapping, for example, an Elemental eigensolve with unrelated parallel computation, which is desirable within a DFT code.  Alternatively, MADNESS could use MULTIPLE so that MPI takes care of thread-safety, but this is unnecessary outside of the times when MADNESS and Elemental overlap and it is generally the case that MULTIPLE leads to decreased performance of MPI relative to SERIALIZED.  In some cases, this overhead is substantial, which is one of the reasons that MADNESS was designed to avoid using MULTIPLE in the first place.  Finally, Elemental could employ the MADNESS lock for all of its MPI calls, but this is a software engineering nightmare since this philosophy would require Elemental to support external locking interfaces for every application, library or runtime with which it was to interoperate, which could be a rather large set.  In theory, Elemental could support an abstract interface for the user to provide their own functions to access an external lock, but is not an appealing solution, nor one which can be generally assumed exists in all external libraries that MADNESS might want to use.  Another solution would be to extend MPI to support temporally local mutex calls to achieve MULTIPLE without the overhead in every MPI function call, but changes to the MPI standard take time and should be used only a means of last resort for application software challenges.

A second example is between MPI and other communication runtimes.  The [NWChem](http://www.nwchem-sw.org/) quantum chemistry code uses Global Arrays(GA) for communication.  GA developed over the same period as MPI and provides a similar but non-overlapping set of communication operations as MPI, especially one-sided primitives required by the GA programming model, which is bulk-synchronous parallel (BSP) but asynchronous and dynamically load-balanced within each BSP region.  The one-sided operations in GA are found in [ARMCI](https://www.emsl.pnl.gov/docs/parsoft/armci/).  On modern systems that support RDMA (remote direct memory access), ARMCI and MPI both utilize registered memory segments and low-level put and get operations. While systems such as Blue Gene/P have relatively simple memory registration, which is both fast and permits overlapping regions, Infiniband memory registration is not fast, hence a good implementation of MPI or ARMCI will cache the registered segments. Unfortunately, this can lead to exhaustion of available registered memory when the application memory use is high and other factors (e.g. page fragmentation) causes registration requests to fail.  This is far from an academic problem; users are now recommended to disable InfiniBand support within MPI on Infiniband systems, which has a huge impact on performance of MPI operations.  While ARMCI performs the bulk of the communication in many NWChem calculations, those that use [ScaLAPACK](http://www.netlib.org/scalapack) or parallel 3D FFT also make heavy use of MPI point-to-point and collectives, which are going to run significantly slower when running in TCP/IP mode over InfiniBand as compared to the native implementation.

There are again multiple solutions to the runtime interoperability problem between MPI and ARMCI on InfiniBand.  One could implement a library for managing registered memory that would be shared between both, but this would require buy-in from Open-MPI, MVAPICH2, Intel MPI and other implementations of MPI for InfiniBand, as well as from ARMCI.  Such a library does not exist in the context of MPI or ARMCI; GASNet appears to have abstracted this as Firehose, at least internally, but it is not used by another other communication middleware.  A second solution is for the ARMCI and MPI interfaces to be implemented within a single runtime, but this is a nontrivial implementation effort due to the enormity of the MPI standard and the lack of an equivalently well-defined specification for ARMCI.

Finally, one can implement ARMCI in terms of MPI or vice verse. Implementing MPI over ARMCI is impossible due to the lack of process management routines in ARMCI, but ARMCI over MPI has been implemented in multiple ways - first using MPI two-sided in the canonical implementation of ARMCI, and second using MPI one-sided by Dinan and coworkers.

One of the primary goals of MPI is to enable both applications and libraries, especially in the context of good software engineering practices.  As seen here, combining MPI with threads or another communication runtime can create interoperability challenges.  These are certainly not insoluble, but they may require substantial levels of effort to workaround if the application end-user quality-of-service, which obviously includes performance, is to be unperturbed.  Application developers are encouraged to think carefully about the evolution of their codes in order to enable the use of scalable and robust external libraries and multiple runtimes to deal with ever more complex HPC hardware.

# About the Author (in 2012) 

Jeff Hammond is an Assistant Computational Scientist at the ALCF, where he is developing computational chemistry software and communication subsystems for Blue Gene/P and Q. He is a past Directorâ€™s Postdoctoral Fellow at Argonne. Hammond received his Ph.D. in Chemistry from the University of Chicago as a DOE Computational Science Graduate Fellow. While in graduate school, Jeff made significant contributions to NWChem, including molecular property functionality for many-body methods. He holds degrees in Chemistry and Mathematics from the University of Washington.
